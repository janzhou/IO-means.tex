\section{Related Work}

Since a computational model for neural networks was created in 1940s~\cite{McCulloch1943},
lots of research has been devoted into this approach~\cite{pouyanfar2018survey}.
Though it was theoretically proven to be a powerful model which was able to approximate any interest function to any degree of accuracy
Hornik et. al.~\cite{Hornik1989}, it did not exert its full strength until last decade due to the lack of enough data and computational resource.
As those issues has been recently addressed by collecting suitable datasets~\cite{JiaDeng2009}
and introducing specialized computing devices~\cite{Krizhevsky2012, Jouppi2017},
it has achieved breakthrough performance on a variety of research problems~\cite{Krizhevsky2012, Mnih2015, Silver2016, Bahdanau2014}.

\subsection{Traditional I/O Prefetch}

Because secondary storage is physically farthest away from CPU in modern memory hierarchy
and shared by many concurrent processes,
I/O accesses bear less locality and is more complexity than that of memory workloads~\cite{yang1992novel, wang1995cat}.
Much research has been done to employ prefetch as one weapon to bridge the speed gap between memory and storage.
However, storage side prefetching mainly optimized for relatively simple I/O patterns.
DiskSeen~\cite{ding2007diskseen} adopts sequential prefetching for disk I/O.
Competitive prefetching~\cite{li2007competitive} optimizes the sequential prefetching for multiple concurrent I/O stream.
In order to optimize the workloads that are not sequential,
Chen et. al.~\cite{chen2008hiding} perform pre-execution and keep a record of the request for prefetching.
Signature-based parallel prefetching~\cite{byna2008parallel} calculates the signature of request sequence to match future request sequence. %signature-based prefetching.
Lots of researchers develop solutions to learn patterns for
specific workloads that are characterized by stable and repeatable patterns.
For example, Anwar et. al.~\cite{anwar2018improving} seek to perform
prefetching specifically for docker applications. % simpler patterns
Xu et. al.~\cite{xu2017malware} learn access patterns to detect
the malware applications.
Even more researchers are using deep learning~\cite{hashemi2018learning, peled2018towards} to implement CPU prefetch,
how to use the deep learning for I/O architecture remains an under-explored research area.

\subsection{Recurrent Neural Network}

Given a sequence of inputs $(x_1, ..., x_T)$, a Recurrent Neural Network (RNN)~\cite{rumelhart1986learning, werbos1990backpropagation} computes a sequence of outputs $(y_1, ..., y_T)$ by iterating the Equation~\ref{eq:rnn1} and \ref{eq:rnn2}:

\begin{equation}
\label{eq:rnn1}
h_t = sigm(W^{hx}x_t + W^{hh}h_{t-1})
\end{equation}

\begin{equation}
\label{eq:rnn2}
y_t = W^{yh}h_t
\end{equation}

The RNN can be applyied to predict the future data requests~\cite{hashemi2018learning, peled2018towards}.
However, as the address space of an external I/O stream is extremely sparse,
the I/O prediction would need a wide neural network, which is hard to train and will not produce accurate output.
Also, the RNN requires explicity patterns exists in the datasets.
However, interference between multiple concurrent I/O stream would hide those patterns.