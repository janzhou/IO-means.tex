\section{Related Work}

Since a computational model for neural networks was created in 1940s~\cite{McCulloch1943},
lots of research has been devoted into this approach~\cite{pouyanfar2018survey}.
Though it was theoretically proven to be a powerful model which was able to approximate any interest function to any degree of accuracy
Hornik et. al.~\cite{Hornik1989}, it did not exert its full strength until last decade due to the lack of enough data and computational resource.
As those issues has been recently addressed by collecting suitable datasets~\cite{JiaDeng2009}
and introducing specialized computing devices~\cite{Krizhevsky2012, Jouppi2017},
it has achieved breakthrough performance on a variety of research problems~\cite{Krizhevsky2012, Mnih2015, Silver2016, Bahdanau2014}.

\subsection{Recurrent neural network}

Given a sequence of inputs $(x_1, ..., x_T)$, a Recurrent Neural Network (RNN)~\cite{rumelhart1986learning, werbos1990backpropagation} computes a sequence of outputs $(y_1, ..., y_T)$ by iterating the Equation~\ref{eq:rnn1} and \ref{eq:rnn2}:

\begin{equation}
\label{eq:rnn1}
h_t = sigm(W^{hx}x_t + W^{hh}h_{t-1})
\end{equation}

\begin{equation}
\label{eq:rnn2}
y_t = W^{yh}h_t
\end{equation}

The RNN can easily sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time.
It can be applyied to I/O sequence data to predict the future requests.
However, as the address space of an I/O stream is extremely sparse,
training a wide RNN for I/O prediction is time consuming and will not get accurate output.
Also, the RNN requires explicity patterns exists in the datasets.
However, interference exists between multiple concurrent I/O stream.

the address space Learning memory access patterns~\cite{hashemi2018learning, peled2018towards}.

Estimate application (Malware) by learning memory access patterns~\cite{xu2017malware}.

Memory Learning framework~\cite{deng2017memory} and Intel Optane~\cite{elie2018intel}.

The case for learned index~\cite{kraska2018case}.

Prefetch for docker~\cite{anwar2018improving}.

Bridge Application Gap by FStream~\cite{rho2018fstream}


\subsection{I/O prefetch as sequence prediction problem}

Methods used in Memory Access Pattern Learning may not work.

\subsubsection{Analysis results for $\delta$}

Frequently Appeared Deltas: Highly skewed

\subsubsection{Analysis results for $classification$}

Clustering: High transit rate for External I/O (Using K-means)

One or more application doing external I/O concurrently.

\subsection{Recurrent Sequence to Sequence Learning}

\subsubsection{Embedding LSTM}

\subsubsection{Clustering LSTM}

Address clustering.

\subsubsection{Multi-layer LSTM}

We first predict the partition (RNN Layer 1) or maybe stream clustering rather than address clustering.

We then predict with in the partition (RNN Layer 2).

\subsubsection{Disjoint Classification}

Convert 1 dimensional sequence to 2 dimensional access jumps

Class disjoint space (jumps) using 2 dimensional k-means

RNN in each class.

