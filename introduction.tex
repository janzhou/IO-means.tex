\section{Introduction}

Because secondary storage is physically farthest away from CPU in modern memory hierarchy
and shared by many concurrent processes,
I/O accesses bear less locality and is more complexity than that of memory workloads~\cite{yang1992novel}.
Much research has been done to employ prefetch as one weapon to bridge the speed gap between memory and storage.
However, storage side prefetching mainly optimized for relatively simple I/O patterns.
DiskSeen~\cite{ding2007diskseen} adopts sequential prefetching for disk I/O.
Competitive prefetching~\cite{li2007competitive} optimizes the sequential prefetching for multiple concurrent I/O stream.
In order to optimize the workloads that are not sequential,
Chen et. al.~\cite{chen2008hiding} perform pre-execution and keep a record of the request for prefetching.
Signature-based parallel prefetching~\cite{byna2008parallel} calculates the signature of request sequence to match future request sequence. %signature-based prefetching.
Lots of researchers develop solutions to learn patterns for
specific workloads that are characterized by stable and repeatable patterns.
For example, Anwar et. al.~\cite{anwar2018improving} seek to perform
prefetching specifically for docker applications. % simpler patterns
Xu et. al.~\cite{xu2017malware} learn access patterns to detect
the malware applications.
Even more researchers are using deep learning~\cite{hashemi2018learning} to implement CPU prefetch,
how to use the deep learning for I/O architecture remains an under-explored research area.
There are challenges to predict future I/Os for the storage systems.
Hence, levering advanced machine learning algorithms for I/O prediction and prefetching
are becoming important for next generate storage systems.

It is common for thousands of applications running concurrently
on a shared cluster~\cite{stokely2012projecting}.
Each application generates its own specific pattern of workloads.
This could result in a wide variety of aggregated
request arrival patterns with high variation over short time intervals~\cite{reiss2012heterogeneity}.
Due to the high degree of both heterogeneity and dynamism in the workloads,
it is difficult to achieve accurate prediction of I/O requests and resource demands.
As thousands of applications are sharing one single storage infrastructure,
I/O sequence from multiple applications are mixed into a single consolidated I/O sequence,
resulting in many chaos and one time access patterns.
Becase the aggregated external I/O requests are far more complex than the memory access.
traditional clustering, classification and prediction techniques
are not suitable for the resource allocation of storage systems~\cite{ray2017high}.
It is challenge to identify the workload in real time from mixed I/O streams.
For example, deep learning is effective in predicting the sequence datasets.
It assumes explicit patterns exists in input sequence,
then exploits such patterns from the training datasets and deduce inference models.
Because of multiple concurrent I/O workloads,
address requests sequence in storage system do not simply carry such info.
To solve these challenges, one solution is to let the application submit it's identity, such file id or process id, to the learning system.
However, we may access multiple files or process multiple transactions in one application.
As a result, passing all the ids to the storage layer will becoming infeasible in real systems.

Another challenge to apply deep learning in storage system is how to predict on large address space.
Two most representative machine learning techniques,
Recurrent Neural Network (RNN in brief) for natural language processing~\cite{graves2013generating,cho2014learning,li2015constructing}
and Convolutional Neural Network (CNN in brief) for image classification~\cite{lecun1995convolutional,karpathy2014large,kim2014convolutional,abdel2014convolutional,krizhevsky2012imagenet},
have been well recognized by the public community.
In natural language processing, the vocabulary size is
typically set to a number ranged from $10^4$ to $10^5$~\cite{Britz:2017}.
The ImageNet~\cite{deng2009imagenet}, one of the most popular image datasets, has $32,326$ image categorizes.
Both DL models work well at a scale with hundreds of thousands of dimensions.
However, the scale of dimension for storage systems is at an extreme scale,
often as large as tera-scale and peta-scale.
For example, given a single 1TB SSD storage device,
there are more than $1.3*10^8$ logical pages,
not talking about the large-scale storage systems.
Due to the large capacity of the external storage,
the corresponding neural network must be wide in order to be effectively trained
and used to make an accurate prediction.
The I/O data access pattern cannot be the effectively learned
by existing CNN or RNN models.

Sequence isolation is one of the sequence classification problem.
However, one of the most important question is which feature we can extract from the I/O sequence to be used for classification.
To reduce the size of the neural network, Google propose to do k-means classification for the addresses,
before submitting the I/O sequence to a set of RNN model\cite{hashemi2018learning}.
However, classification methods used in memory access pattern have strong assumptions and will not work for external I/O.
First, frequently appeared deltas are too highly skewed in the external I/O.
Second, because of mix nature of the I/O sequence, the transit rate between k-means clusters is too high for external I/O.
We will develop a more general solution to find the data relationship to those data in last slide time window.
We aim to discover the hidden relation being buried in address domain,
and project our trace records to a new multi feature domain by considering time series info.
The convergence algorithm is being developed to implement a projection protocol
taking both address and time series info of data into consideration.

% the address space Learning memory access patterns~\cite{hashemi2018learning, peled2018towards}.