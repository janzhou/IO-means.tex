@article{hashemi2018learning,
  title={Learning Memory Access Patterns},
  author={Hashemi, Milad and Swersky, Kevin and Smith, Jamie A and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  journal={arXiv preprint arXiv:1803.02329},
  year={2018}
}

@inproceedings{kraska2018case,
  title={The case for learned index structures},
  author={Kraska, Tim and Beutel, Alex and Chi, Ed H and Dean, Jeffrey and Polyzotis, Neoklis},
  booktitle={Proceedings of the 2018 International Conference on Management of Data},
  pages={489--504},
  year={2018},
  organization={ACM}
}

@inproceedings{xu2017malware,
  title={Malware detection using machine learning based analysis of virtual memory access patterns},
  author={Xu, Zhixing and Ray, Sayak and Subramanyan, Pramod and Malik, Sharad},
  booktitle={Proceedings of the Conference on Design, Automation \& Test in Europe},
  pages={169--174},
  year={2017},
  organization={European Design and Automation Association}
}

@article{peled2018towards,
  title={Towards Memory Prefetching with Neural Networks: Challenges and Insights},
  author={Peled, Leeor and Weiser, Uri and Etsion, Yoav},
  journal={arXiv preprint arXiv:1804.00478},
  year={2018}
}

@inproceedings{deng2017memory,
  title={Memory cocktail therapy: a general learning-based framework to optimize dynamic tradeoffs in NVMs},
  author={Deng, Zhaoxia and Zhang, Lunkai and Mishra, Nikita and Hoffmann, Henry and Chong, Frederic T},
  booktitle={Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={232--244},
  year={2017},
  organization={ACM}
}

@inproceedings{elie2018intel,
  title={Intel Optane™ technology as differentiator for internet of everything and fog computing},
  author={Elie, Etienne},
  booktitle={Software Defined Systems (SDS), 2018 Fifth International Conference on},
  pages={3--3},
  year={2018},
  organization={IEEE}
}

@inproceedings{chen2017research,
  title={Research and Realization of Commodity Image Retrieval System Based on Deep Learning},
  author={Chen, Cen and Yang, Rui and Wang, Chongwen},
  booktitle={International Symposium on Parallel Architecture, Algorithm and Programming},
  pages={376--385},
  year={2017},
  organization={Springer}
}

@article{kang2017reinforcement,
  title={Reinforcement Learning-Assisted Garbage Collection to Mitigate Long-Tail Latency in SSD},
  author={Kang, Wonkyung and Shin, Dongkun and Yoo, Sungjoo},
  journal={ACM Transactions on Embedded Computing Systems (TECS)},
  volume={16},
  number={5s},
  pages={134},
  year={2017},
  publisher={ACM}
}

@inproceedings{cano2017curator,
  title={Curator: Self-Managing Storage for Enterprise Clusters.},
  author={Cano, Ignacio and Aiyar, Srinivas and Arora, Varun and Bhattacharyya, Manosiz and Chaganti, Akhilesh and Cheah, Chern and Chun, Brent N and Gupta, Karan and Khot, Vinayak and Krishnamurthy, Arvind},
  booktitle={NSDI},
  pages={51--66},
  year={2017}
}

@inproceedings{cano2017curator,
  title={Curator: Self-Managing Storage for Enterprise Clusters.},
  author={Cano, Ignacio and Aiyar, Srinivas and Arora, Varun and Bhattacharyya, Manosiz and Chaganti, Akhilesh and Cheah, Chern and Chun, Brent N and Gupta, Karan and Khot, Vinayak and Krishnamurthy, Arvind},
  booktitle={NSDI},
  pages={51--66},
  year={2017}
}

@article{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  journal={arXiv preprint arXiv:1705.03122},
  year={2017}
}

@article{mao2017exploring,
  title={Exploring the regularity of sparse structure in convolutional neural networks},
  author={Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1705.08922},
  year={2017}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{quackenbush2018beyond,
  title={Beyond Profiling},
  author={Quackenbush, Chris and Zahran, Mohamed},
  journal={The 1st International Workshop on AI-assisted Design for Architecture},
  year={2018}
}

@article{McCulloch1943,
  abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {McCulloch, Warren S. and Pitts, Walter},
  doi = {10.1007/BF02478259},
  eprint = {arXiv:1011.1669v3},
  file = {:home/zxi/Documents/Mendeley Desktop/Of, Biophysics{\_}1943{\_}IDEAS IMMANENT IN NERVOUS ACTIVITY I . Introduction neuron m a y be excited by impulses a r r i v i n g at a suffici.pdf:pdf},
  isbn = {0007-4985},
  issn = {00074985},
  journal = {The Bulletin of Mathematical Biophysics},
  number = {4},
  pages = {115--133},
  pmid = {2185863},
  title = {{A logical calculus of the ideas immanent in nervous activity}},
  volume = {5},
  year = {1943}
}

@article{Hornik1989,
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
  archivePrefix = {arXiv},
  arxivId = {arXiv:1011.1669v3},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  doi = {10.1016/0893-6080(89)90020-8},
  eprint = {arXiv:1011.1669v3},
  file = {:home/zxi/Documents/Mendeley Desktop/Hornik, Stinchcombe, White{\_}1989{\_}Multilayer feedforward networks are universal approximators.pdf:pdf},
  isbn = {08936080 (ISSN)},
  issn = {08936080},
  journal = {Neural Networks},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  number = {5},
  pages = {359--366},
  pmid = {74},
  title = {{Multilayer feedforward networks are universal approximators}},
  volume = {2},
  year = {1989}
}

@article{JiaDeng2009,
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
  doi = {10.1109/CVPRW.2009.5206848},
  file = {:home/zxi/Documents/Mendeley Desktop/Deng et al.{\_}Unknown{\_}ImageNet A Large-Scale Hierarchical Image Database.pdf:pdf},
  isbn = {978-1-4244-3992-8},
  issn = {1063-6919},
  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages = {248--255},
  pmid = {21914436},
  title = {{ImageNet: A large-scale hierarchical image database}},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
  year = {2009}
}

@article{Krizhevsky2012,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
  archivePrefix = {arXiv},
  arxivId = {1102.0183},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
  eprint = {1102.0183},
  file = {:home/zxi/Documents/Mendeley Desktop/Krizhevsky, Hinton{\_}Unknown{\_}ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
  isbn = {9781627480031},
  issn = {10495258},
  journal = {Advances In Neural Information Processing Systems},
  pages = {1--9},
  pmid = {7491034},
  title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  year = {2012}
}

@article{Jouppi2017,
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a ​ Tensor Processing Unit (TPU) ​ — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, {\ldots}) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95{\%} of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. Index terms–DNN, MLP, CNN, RNN, LSTM, neural network, domain-specific architecture, accelerator},
  archivePrefix = {arXiv},
  arxivId = {1704.04760},
  author = {Jouppi, Norman P. and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Young, Cliff and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Patil, Nishant and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Patterson, David and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Agrawal, Gaurav and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Bajwa, Raminder and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Bates, Sarah and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun and Bhatia, Suresh and Boden, Nan},
  doi = {10.1145/3140659.3080246},
  eprint = {1704.04760},
  file = {:home/zxi/Documents/Mendeley Desktop/Jouppi et al.{\_}Unknown{\_}In - Datacenter Performance Analysis of a Tensor Processing Unit NETWORKS.pdf:pdf},
  isbn = {9781450348928},
  issn = {01635964},
  journal = {ACM SIGARCH Computer Architecture News},
  keywords = {accelerator,acm reference format,c architecture,cnn,deep learning,dnn,domain - speci fi,gpu,lstm,mlp,n eural network,rnn,tensorflow,tpu},
  number = {2},
  pages = {1--12},
  title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
  url = {http://dl.acm.org/citation.cfm?doid=3140659.3080246},
  volume = {45},
  year = {2017}
}

@article{Mnih2015,
  abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
  archivePrefix = {arXiv},
  arxivId = {1312.5602},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  doi = {10.1038/nature14236},
  eprint = {1312.5602},
  isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  issn = {14764687},
  journal = {Nature},
  keywords = {neural networks,reinforcement learning},
  mendeley-tags = {neural networks,reinforcement learning},
  number = {7540},
  pages = {529--533},
  pmid = {25719670},
  title = {{Human-level control through deep reinforcement learning}},
  volume = {518},
  year = {2015}
}

@article{Silver2016,
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  archivePrefix = {arXiv},
  arxivId = {1610.00633},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  eprint = {1610.00633},
  file = {:home/zxi/Documents/Mendeley Desktop/Silver et al.{\_}2016{\_}Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
  isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
  issn = {14764687},
  journal = {Nature},
  number = {7587},
  pages = {484--489},
  pmid = {26819042},
  publisher = {Nature Publishing Group},
  title = {{Mastering the game of Go with deep neural networks and tree search}},
  url = {http://dx.doi.org/10.1038/nature16961},
  volume = {529},
  year = {2016}
}

@article{Bahdanau2014,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archivePrefix = {arXiv},
  arxivId = {1409.0473},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  doi = {10.1146/annurev.neuro.26.041002.131047},
  eprint = {1409.0473},
  file = {:home/zxi/Documents/Mendeley Desktop/Bahdanau, Cho, Bengio{\_}2015{\_}N EURAL M ACHINE T RANSLATION.pdf:pdf},
  isbn = {0147-006X (Print)},
  issn = {0147-006X},
  pages = {1--15},
  pmid = {14527267},
  title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
  url = {http://arxiv.org/abs/1409.0473},
  year = {2014}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@inproceedings{yu2018datasize,
  title={Datasize-Aware High Dimensional Configurations Auto-Tuning of In-Memory Cluster Computing},
  author={Yu, Zhibin and Bei, Zhendong and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={564--577},
  year={2018},
  organization={ACM}
}

@inproceedings{gu2006nexus,
  title={Nexus: a novel weighted-graph-based prefetching algorithm for metadata servers in petabyte-scale storage systems},
  author={Gu, Peng and Zhu, Yifeng and Jiang, Hong and Wang, Jun},
  booktitle={Cluster Computing and the Grid, 2006. CCGRID 06. Sixth IEEE International Symposium on},
  volume={1},
  pages={8--pp},
  year={2006},
  organization={IEEE}
}

@article{oliva2017recurrent,
  title={Recurrent Estimation of Distributions},
  author={Oliva, Junier B and Dubey, Kumar Avinava and Poczos, Barnabas and Xing, Eric and Schneider, Jeff},
  journal={arXiv preprint arXiv:1705.10750},
  year={2017}
}

@article{chung2016hierarchical,
  title={Hierarchical multiscale recurrent neural networks},
  author={Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1609.01704},
  year={2016}
}

@ARTICLE{Britz:2017,
  author          = {{Britz}, D. and {Goldie}, A. and {Luong}, T. and {Le}, Q.},
  title           = "{Massive Exploration of Neural Machine Translation Architectures}",
  journal         = {ArXiv e-prints},
  archivePrefix   = "arXiv",
  eprinttype      = {arxiv},
  eprint          = {1703.03906},
  primaryClass    = "cs.CL",
  keywords        = {Computer Science - Computation and Language},
  year            = 2017,
  month           = mar,
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{graves2013generating,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

@article{pouyanfar2018survey,
  title={A Survey on Deep Learning: Algorithms, Techniques, and Applications},
  author={Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, SS},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={5},
  pages={92},
  year={2018},
  publisher={ACM}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{lecun1995convolutional,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@inproceedings{karpathy2014large,
  title={Large-scale video classification with convolutional neural networks},
  author={Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={1725--1732},
  year={2014}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{abdel2014convolutional,
  title={Convolutional neural networks for speech recognition},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={IEEE/ACM Transactions on audio, speech, and language processing},
  volume={22},
  number={10},
  pages={1533--1545},
  year={2014},
  publisher={IEEE}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{li2015constructing,
  title={Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition},
  author={Li, Xiangang and Wu, Xihong},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
  pages={4520--4524},
  year={2015},
  organization={IEEE}
}

@article{dahl2012context,
  title={Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition},
  author={Dahl, George E and Yu, Dong and Deng, Li and Acero, Alex},
  journal={IEEE Transactions on audio, speech, and language processing},
  volume={20},
  number={1},
  pages={30--42},
  year={2012},
  publisher={IEEE}
}

@article{kolbk2017speech,
  title={Speech intelligibility potential of general and specialized deep neural network based speech enhancement systems},
  author={Kolbk, Morten and Tan, Zheng-Hua and Jensen, Jesper and Kolbk, Morten and Tan, Zheng-Hua and Jensen, Jesper},
  journal={IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
  volume={25},
  number={1},
  pages={153--167},
  year={2017},
  publisher={IEEE Press}
}

@article{choy2006neural,
  title={Neural networks for continuous online learning and control},
  author={Choy, Min Chee and Srinivasan, Dipti and Cheu, Ruey Long},
  journal={IEEE Transactions on Neural Networks},
  volume={17},
  number={6},
  pages={1511--1531},
  year={2006},
  publisher={IEEE}
}
